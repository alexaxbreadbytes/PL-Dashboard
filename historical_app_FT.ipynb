{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f34154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import streamlit as st\n",
    "import plotly.express as px\n",
    "import altair as alt\n",
    "import dateutil.parser\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebc1657a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 22:14:06.233 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /usr/local/Caskroom/miniconda/base/envs/bbytes/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "ename": "InternalHashError",
     "evalue": "module '__main__' has no attribute '__file__'\n\nWhile caching the body of `load_data()`, Streamlit encountered an\nobject of type `builtins.function`, which it does not know how to hash.\n\n**In this specific case, it's very likely you found a Streamlit bug so please\n[file a bug report here.]\n(https://github.com/streamlit/streamlit/issues/new/choose)**\n\nIn the meantime, you can try bypassing this error by registering a custom\nhash function via the `hash_funcs` keyword in @st.cache(). For example:\n\n```\n@st.cache(hash_funcs={builtins.function: my_hash_func})\ndef my_func(...):\n    ...\n```\n\nIf you don't know where the object of type `builtins.function` is coming\nfrom, try looking at the hash chain below for an object that you do recognize,\nthen pass that to `hash_funcs` instead:\n\n```\nObject of type builtins.function: <function load_data at 0x11a9a5800>\n```\n\nPlease see the `hash_funcs` [documentation]\n(https://docs.streamlit.io/library/advanced-features/caching#the-hash_funcs-parameter)\nfor more details.\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/bbytes/lib/python3.11/site-packages/streamlit/runtime/legacy_caching/hashing.py:360\u001b[0m, in \u001b[0;36m_CodeHasher.to_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;66;03m# Hash the input\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (tname, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;66;03m# call to_bytes inside _to_bytes things get double-counted.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/bbytes/lib/python3.11/site-packages/streamlit/runtime/legacy_caching/hashing.py:626\u001b[0m, in \u001b[0;36m_CodeHasher._to_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m code \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file_should_be_hashed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_filename\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    627\u001b[0m     context \u001b[38;5;241m=\u001b[39m _get_context(obj)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/bbytes/lib/python3.11/site-packages/streamlit/runtime/legacy_caching/hashing.py:402\u001b[0m, in \u001b[0;36m_CodeHasher._file_should_be_hashed\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file_util\u001b[38;5;241m.\u001b[39mfile_is_in_folder_glob(\n\u001b[0;32m--> 402\u001b[0m     filepath, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_main_script_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m file_util\u001b[38;5;241m.\u001b[39mfile_in_pythonpath(filepath)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/bbytes/lib/python3.11/site-packages/streamlit/runtime/legacy_caching/hashing.py:714\u001b[0m, in \u001b[0;36m_CodeHasher._get_main_script_directory\u001b[0;34m()\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# This works because we set __main__.__file__ to the\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# script path in ScriptRunner.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m abs_main_path \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[43m__main__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mresolve()\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(abs_main_path\u001b[38;5;241m.\u001b[39mparent)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module '__main__' has no attribute '__file__'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInternalHashError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 315\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    311\u001b[0m     st\u001b[38;5;241m.\u001b[39mset_page_config(\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrading Bot Dashboard\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    313\u001b[0m         layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwide\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    314\u001b[0m     )\n\u001b[0;32m--> 315\u001b[0m     \u001b[43mrunapp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 106\u001b[0m, in \u001b[0;36mrunapp\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m st\u001b[38;5;241m.\u001b[39msubheader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChoose your settings:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m no_errors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m df \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCT-Trade-Log.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# so as not to mutate cached value \u001b[39;00m\n\u001b[1;32m    108\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrade\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntry Date\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuy Price\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSell Price\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExit Date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP/L per token\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP/L \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDrawdown \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    109\u001b[0m df\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSignal\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLong\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(df)) \n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/bbytes/lib/python3.11/site-packages/streamlit/runtime/legacy_caching/caching.py:624\u001b[0m, in \u001b[0;36mcache.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show_spinner:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m spinner(message):\n\u001b[0;32m--> 624\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_or_create_cached_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_or_create_cached_value()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/bbytes/lib/python3.11/site-packages/streamlit/runtime/legacy_caching/caching.py:549\u001b[0m, in \u001b[0;36mcache.<locals>.wrapped_func.<locals>.get_or_create_cached_value\u001b[0;34m()\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mnonlocal\u001b[39;00m cache_key\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;66;03m# Delay generating the cache key until the first call.\u001b[39;00m\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;66;03m# This way we can see values of globals, including functions\u001b[39;00m\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;66;03m# defined after this one.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;66;03m# If we generated the key earlier we would only hash those\u001b[39;00m\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;66;03m# globals by name, and miss changes in their code or value.\u001b[39;00m\n\u001b[0;32m--> 549\u001b[0m     cache_key \u001b[38;5;241m=\u001b[39m \u001b[43m_hash_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnon_optional_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_funcs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;66;03m# First, get the cache that's attached to this function.\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# This cache's key is generated (above) from the function's code.\u001b[39;00m\n\u001b[1;32m    553\u001b[0m mem_cache \u001b[38;5;241m=\u001b[39m _mem_caches\u001b[38;5;241m.\u001b[39mget_cache(cache_key, max_entries, ttl)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/bbytes/lib/python3.11/site-packages/streamlit/runtime/legacy_caching/caching.py:676\u001b[0m, in \u001b[0;36m_hash_func\u001b[0;34m(func, hash_funcs)\u001b[0m\n\u001b[1;32m    665\u001b[0m update_hash(\n\u001b[1;32m    666\u001b[0m     (func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m),\n\u001b[1;32m    667\u001b[0m     hasher\u001b[38;5;241m=\u001b[39mfunc_hasher,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    670\u001b[0m     hash_source\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    671\u001b[0m )\n\u001b[1;32m    673\u001b[0m \u001b[38;5;66;03m# Include the function's body in the hash. We *do* pass hash_funcs here,\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;66;03m# because this step will be hashing any objects referenced in the function\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# body.\u001b[39;00m\n\u001b[0;32m--> 676\u001b[0m \u001b[43mupdate_hash\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhasher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_hasher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhash_funcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhash_funcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhash_reason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHashReason\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCACHING_FUNC_BODY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhash_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m cache_key \u001b[38;5;241m=\u001b[39m func_hasher\u001b[38;5;241m.\u001b[39mhexdigest()\n\u001b[1;32m    684\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmem_cache key for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m, cache_key\n\u001b[1;32m    686\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/bbytes/lib/python3.11/site-packages/streamlit/runtime/legacy_caching/hashing.py:108\u001b[0m, in \u001b[0;36mupdate_hash\u001b[0;34m(val, hasher, hash_reason, hash_source, context, hash_funcs)\u001b[0m\n\u001b[1;32m    105\u001b[0m hash_stacks\u001b[38;5;241m.\u001b[39mcurrent\u001b[38;5;241m.\u001b[39mhash_source \u001b[38;5;241m=\u001b[39m hash_source\n\u001b[1;32m    107\u001b[0m ch \u001b[38;5;241m=\u001b[39m _CodeHasher(hash_funcs)\n\u001b[0;32m--> 108\u001b[0m \u001b[43mch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhasher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/bbytes/lib/python3.11/site-packages/streamlit/runtime/legacy_caching/hashing.py:385\u001b[0m, in \u001b[0;36m_CodeHasher.update\u001b[0;34m(self, hasher, obj, context)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, hasher, obj: Any, context: Optional[Context] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;124;03m\"\"\"Update the provided hasher with the hash of an object.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m     hasher\u001b[38;5;241m.\u001b[39mupdate(b)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/bbytes/lib/python3.11/site-packages/streamlit/runtime/legacy_caching/hashing.py:374\u001b[0m, in \u001b[0;36m_CodeHasher.to_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InternalHashError(ex, obj)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# In case an UnhashableTypeError (or other) error is thrown, clean up the\u001b[39;00m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;66;03m# stack so we don't get false positives in future hashing calls\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     hash_stacks\u001b[38;5;241m.\u001b[39mcurrent\u001b[38;5;241m.\u001b[39mpop()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/bbytes/lib/python3.11/site-packages/streamlit/runtime/legacy_caching/hashing.py:360\u001b[0m, in \u001b[0;36m_CodeHasher.to_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    356\u001b[0m hash_stacks\u001b[38;5;241m.\u001b[39mcurrent\u001b[38;5;241m.\u001b[39mpush(obj)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;66;03m# Hash the input\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (tname, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;66;03m# call to_bytes inside _to_bytes things get double-counted.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mgetsizeof(b)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/bbytes/lib/python3.11/site-packages/streamlit/runtime/legacy_caching/hashing.py:626\u001b[0m, in \u001b[0;36m_CodeHasher._to_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    624\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__code__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m code \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file_should_be_hashed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_filename\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    627\u001b[0m     context \u001b[38;5;241m=\u001b[39m _get_context(obj)\n\u001b[1;32m    628\u001b[0m     defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__defaults__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/bbytes/lib/python3.11/site-packages/streamlit/runtime/legacy_caching/hashing.py:402\u001b[0m, in \u001b[0;36m_CodeHasher._file_should_be_hashed\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_is_blacklisted:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file_util\u001b[38;5;241m.\u001b[39mfile_is_in_folder_glob(\n\u001b[0;32m--> 402\u001b[0m     filepath, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_main_script_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m file_util\u001b[38;5;241m.\u001b[39mfile_in_pythonpath(filepath)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/bbytes/lib/python3.11/site-packages/streamlit/runtime/legacy_caching/hashing.py:714\u001b[0m, in \u001b[0;36m_CodeHasher._get_main_script_directory\u001b[0;34m()\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01m__main__\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# This works because we set __main__.__file__ to the\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# script path in ScriptRunner.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m abs_main_path \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[43m__main__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mresolve()\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(abs_main_path\u001b[38;5;241m.\u001b[39mparent)\n",
      "\u001b[0;31mInternalHashError\u001b[0m: module '__main__' has no attribute '__file__'\n\nWhile caching the body of `load_data()`, Streamlit encountered an\nobject of type `builtins.function`, which it does not know how to hash.\n\n**In this specific case, it's very likely you found a Streamlit bug so please\n[file a bug report here.]\n(https://github.com/streamlit/streamlit/issues/new/choose)**\n\nIn the meantime, you can try bypassing this error by registering a custom\nhash function via the `hash_funcs` keyword in @st.cache(). For example:\n\n```\n@st.cache(hash_funcs={builtins.function: my_hash_func})\ndef my_func(...):\n    ...\n```\n\nIf you don't know where the object of type `builtins.function` is coming\nfrom, try looking at the hash chain below for an object that you do recognize,\nthen pass that to `hash_funcs` instead:\n\n```\nObject of type builtins.function: <function load_data at 0x11a9a5800>\n```\n\nPlease see the `hash_funcs` [documentation]\n(https://docs.streamlit.io/library/advanced-features/caching#the-hash_funcs-parameter)\nfor more details.\n            "
     ]
    }
   ],
   "source": [
    "@st.experimental_memo\n",
    "def get_hist_info(df_coin, principal_balance,plheader):\n",
    "    numtrades = int(len(df_coin))\n",
    "    numwin = int(sum(df_coin[plheader] > 0))\n",
    "    numloss = int(sum(df_coin[plheader] < 0))\n",
    "    winrate = int(np.round(100*numwin/numtrades,2))\n",
    "    \n",
    "    grosswin = sum(df_coin[df_coin[plheader] > 0][plheader])\n",
    "    grossloss = sum(df_coin[df_coin[plheader] < 0][plheader])\n",
    "    if grossloss !=0:\n",
    "        pfactor = -1*np.round(grosswin/grossloss,2)\n",
    "    else: \n",
    "        pfactor = np.nan\n",
    "    return numtrades, numwin, numloss, winrate, pfactor\n",
    "@st.experimental_memo\n",
    "def get_rolling_stats(df, lev, otimeheader, days):\n",
    "    rollend = datetime.today()-timedelta(days=days)\n",
    "    rolling_df = df[df[otimeheader] >= rollend]\n",
    "\n",
    "    if len(rolling_df) > 0:\n",
    "        rolling_perc = rolling_df['Return Per Trade'].dropna().cumprod().values[-1]-1\n",
    "    else: \n",
    "        rolling_perc = 0\n",
    "    return 100*lev*rolling_perc\n",
    "\n",
    "@st.experimental_memo\n",
    "def filt_df(df, cheader, symbol_selections):\n",
    "    \"\"\"\n",
    "        Inputs: df (pd.DataFrame), cheader (str) and symbol_selections (list[str]).\n",
    "        \n",
    "        Returns a filtered pd.DataFrame containing only data that matches symbol_selections (list[str])\n",
    "        from df[cheader].\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    df = df[df[cheader].isin(symbol_selections)]\n",
    "\n",
    "    return df\n",
    "\n",
    "@st.experimental_memo\n",
    "def my_style(v, props=''):\n",
    "    props = 'color:red' if v < 0 else 'color:green'\n",
    "    return props\n",
    "\n",
    "@st.cache(ttl=24*3600, allow_output_mutation=True)\n",
    "def load_data(filename, otimeheader,fmat):\n",
    "    df = pd.read_csv(open(filename,'r'), sep='\\t') # so as not to mutate cached value \n",
    "    df.columns = ['Trade','Signal','Entry Date','Buy Price', 'Sell Price','Exit Date', 'P/L per token', 'P/L %']\n",
    "    \n",
    "    df['Buy Price'] = df['Buy Price'].str.replace('$', '', regex=True)\n",
    "    df['Sell Price'] = df['Sell Price'].str.replace('$', '', regex=True)\n",
    "    df['Buy Price'] = df['Buy Price'].str.replace(',', '', regex=True)\n",
    "    df['Sell Price'] = df['Sell Price'].str.replace(',', '', regex=True)\n",
    "    df['P/L per token'] = df['P/L per token'].str.replace('$', '', regex=True)\n",
    "    df['P/L per token'] = df['P/L per token'].str.replace(',', '', regex=True)\n",
    "    df['P/L %'] = df['P/L %'].str.replace('%', '', regex=True)\n",
    "\n",
    "    df['Buy Price'] = pd.to_numeric(df['Buy Price'])\n",
    "    df['Sell Price'] = pd.to_numeric(df['Sell Price'])\n",
    "    df['P/L per token'] = pd.to_numeric(df['P/L per token'])\n",
    "    df['P/L %'] = pd.to_numeric(df['P/L %'])\n",
    "\n",
    "    dateheader = 'Date'\n",
    "    theader = 'Time'\n",
    "\n",
    "    df[dateheader] = [tradetimes.split(\" \")[0] for tradetimes in df[otimeheader].values]\n",
    "    df[theader] = [tradetimes.split(\" \")[1] for tradetimes in df[otimeheader].values]\n",
    "\n",
    "    df[otimeheader]= [dateutil.parser.parse(date+' '+time)\n",
    "                              for date,time in zip(df[dateheader],df[theader])]\n",
    "\n",
    "    df[otimeheader] = pd.to_datetime(df[otimeheader])\n",
    "    df['Exit Date'] = pd.to_datetime(df['Exit Date'])\n",
    "    df.sort_values(by=otimeheader, inplace=True)\n",
    "\n",
    "    df[dateheader] = [dateutil.parser.parse(date).date() for date in df[dateheader]]\n",
    "    df[theader] = [dateutil.parser.parse(time).time() for time in df[theader]]\n",
    "    df['Trade'] = [i+1 for i in range(len(df))] #reindex\n",
    "    \n",
    "    return df\n",
    "\n",
    "def runapp():\n",
    "    bot_selections = \"French Toast\"\n",
    "    otimeheader = 'Entry Date'\n",
    "    plheader = 'P/L %'\n",
    "    fmat = '%Y-%m-%d %H:%M:%S'\n",
    "    dollar_cap = 30000.00\n",
    "    fees = .075/100\n",
    "    st.header(f\"{bot_selections} Performance Dashboard :bread: :moneybag:\")\n",
    "    st.write(\"Welcome to the Trading Bot Dashboard by BreadBytes! You can use this dashboard to track \" +\n",
    "                 \"the performance of our trading bots.\")\n",
    " #   st.sidebar.header(\"FAQ\")\n",
    "\n",
    " #   with st.sidebar.subheader(\"FAQ\"):\n",
    " #       st.write(Path(\"FAQ_README.md\").read_text())\n",
    "    st.subheader(\"Choose your settings:\")\n",
    "    no_errors = True\n",
    "    \n",
    "    data = load_data(\"FT-Trade-Log.csv\",otimeheader,fmat)\n",
    "    df = data.copy(deep=True)\n",
    "    \n",
    "    dateheader = 'Date'\n",
    "    theader = 'Time'\n",
    "\n",
    "    with st.form(\"user input\", ):\n",
    "        if no_errors:\n",
    "            with st.container():\n",
    "                col1, col2 = st.columns(2)\n",
    "                with col1:\n",
    "                    try:\n",
    "                        startdate = st.date_input(\"Start Date\", value=pd.to_datetime(df[otimeheader]).min())\n",
    "                    except:\n",
    "                        st.error(\"Please select your exchange or upload a supported trade log file.\")\n",
    "                        no_errors = False \n",
    "                with col2:\n",
    "                    try:\n",
    "                        enddate = st.date_input(\"End Date\", value=datetime.today())\n",
    "                    except:\n",
    "                        st.error(\"Please select your exchange or upload a supported trade log file.\")\n",
    "                        no_errors = False \n",
    "                #st.sidebar.subheader(\"Customize your Dashboard\")\n",
    "\n",
    "                if no_errors and (enddate < startdate): \n",
    "                    st.error(\"End Date must be later than Start date. Please try again.\")\n",
    "                    no_errors = False \n",
    "            with st.container(): \n",
    "                col1,col2 = st.columns(2) \n",
    "                with col2:\n",
    "                    lev = st.number_input('Leverage', min_value=1, value=1, max_value= 3, step=1)\n",
    "                with col1:\n",
    "                    principal_balance = st.number_input('Starting Balance', min_value=0.00, value=1000.00, max_value= dollar_cap, step=.01)\n",
    "\n",
    "        #hack way to get button centered \n",
    "        c = st.columns(9)\n",
    "        with c[4]: \n",
    "            submitted = st.form_submit_button(\"Get Cookin'!\")           \n",
    "    \n",
    "    if submitted and principal_balance * lev > dollar_cap:\n",
    "        lev = np.floor(dollar_cap/principal_balance)\n",
    "        st.error(f\"WARNING: (Starting Balance)*(Leverage) exceeds the ${dollar_cap} limit. Using maximum available leverage of {lev}\")\n",
    "                \n",
    "    if submitted and no_errors:\n",
    "        df = df[(df[dateheader] >= startdate) & (df[dateheader] <= enddate)]\n",
    "\n",
    "        if len(df) == 0:\n",
    "                st.error(\"There are no available trades matching your selections. Please try again!\")\n",
    "                no_errors = False\n",
    "        if no_errors:\n",
    "\n",
    "            signal_map = {'Long': 1, 'Short':-1} # 1 for long #-1 for short\n",
    "\n",
    "            df['Calculated Return %'] = df['Signal'].map(signal_map)*(1-fees)*((df['Sell Price']-df['Buy Price'])/df['Buy Price'] - fees) #accounts for fees on open and close of trade \n",
    "\n",
    "            df['Return Per Trade'] = 1+df['Calculated Return %'].values\n",
    "\n",
    "            df['Compounded Return'] = df['Return Per Trade'].cumprod()\n",
    "            df['Balance used in Trade'] = [min(dollar_cap/lev, bal*principal_balance) for bal in df['Compounded Return']]\n",
    "            df['Net P/L Per Trade'] = (df['Return Per Trade']-1)*lev*df['Balance used in Trade'] \n",
    "            df['Cumulative P/L'] = df['Net P/L Per Trade'].cumsum()\n",
    "            cum_pl = df.loc[df.dropna().index[-1],'Cumulative P/L'] + principal_balance\n",
    "\n",
    "            effective_return = 100*((cum_pl - principal_balance)/principal_balance)\n",
    "\n",
    "            st.header(f\"{bot_selections} Results\")\n",
    "            if len(bot_selections) > 1:\n",
    "                st.metric(\n",
    "                    \"Total Account Balance\",\n",
    "                    f\"${cum_pl:.2f}\",\n",
    "                    f\"{100*(cum_pl-principal_balance)/(principal_balance):.2f} %\",\n",
    "                )\n",
    "\n",
    "            st.line_chart(data=df.dropna(), x='Exit Date', y='Cumulative P/L', use_container_width=True)\n",
    "\n",
    "            df['Per Trade Return Rate'] = df['Return Per Trade']-1\n",
    "\n",
    "            totals = pd.DataFrame([], columns = ['# of Trades', 'Wins', 'Losses', 'Win Rate', 'Profit Factor'])\n",
    "            data = get_hist_info(df.dropna(), principal_balance,'Per Trade Return Rate')\n",
    "            totals.loc[len(totals)] = list(i for i in data)\n",
    "\n",
    "            totals['Cum. P/L'] = cum_pl-principal_balance\n",
    "            totals['Cum. P/L (%)'] = 100*(cum_pl-principal_balance)/principal_balance\n",
    "            #results_df['Avg. P/L'] = (cum_pl-principal_balance)/results_df['# of Trades'].values[0]\n",
    "            #results_df['Avg. P/L (%)'] = 100*results_df['Avg. P/L'].values[0]/principal_balance\n",
    "\n",
    "            if df.empty:\n",
    "                st.error(\"Oops! None of the data provided matches your selection(s). Please try again.\")\n",
    "            else:\n",
    "                #st.dataframe(totals.style.format({'# of Trades': '{:.0f}','Wins': '{:.0f}','Losses': '{:.0f}','Win Rate': '{:.2f}%','Profit Factor' : '{:.2f}', 'Avg. P/L (%)': '{:.2f}%', 'Cum. P/L (%)': '{:.2f}%', 'Cum. P/L': '{:.2f}', 'Avg. P/L': '{:.2f}'})\n",
    "            #.text_gradient(subset=['Win Rate'],cmap=\"RdYlGn\", vmin = 0, vmax = 100)\\\n",
    "            #.text_gradient(subset=['Profit Factor'],cmap=\"RdYlGn\", vmin = 0, vmax = 2), use_container_width=True)\n",
    "                for row in totals.itertuples():\n",
    "                    col1, col2, col3, col4 = st.columns(4)\n",
    "                    c1, c2, c3, c4 = st.columns(4)\n",
    "                    with col1:\n",
    "                        st.metric(\n",
    "                            \"Total Trades\",\n",
    "                            f\"{row._1:.0f}\",\n",
    "                        )\n",
    "                    with c1:\n",
    "                        st.metric(\n",
    "                            \"Profit Factor\",\n",
    "                            f\"{row._5:.2f}\",\n",
    "                        )\n",
    "                    with col2: \n",
    "                        st.metric(\n",
    "                            \"Wins\",\n",
    "                            f\"{row.Wins:.0f}\",\n",
    "                        )\n",
    "                    with c2:\n",
    "                        st.metric(\n",
    "                            \"Cumulative P/L\",\n",
    "                            f\"${row._6:.2f}\",\n",
    "                            f\"{row._7:.2f} %\",\n",
    "                        )\n",
    "                    with col3: \n",
    "                        st.metric(\n",
    "                            \"Losses\",\n",
    "                            f\"{row.Losses:.0f}\",\n",
    "                        )\n",
    "                    with c3:\n",
    "                        st.metric(\n",
    "                        \"Rolling 7 Days\",\n",
    "                            \"\",#f\"{(1+get_rolling_stats(df,otimeheader, 30))*principal_balance:.2f}\",\n",
    "                            f\"{get_rolling_stats(df,lev, otimeheader, 7):.2f}%\",\n",
    "                        )\n",
    "                        st.metric(\n",
    "                        \"Rolling 30 Days\",\n",
    "                            \"\",#f\"{(1+get_rolling_stats(df,otimeheader, 30))*principal_balance:.2f}\",\n",
    "                            f\"{get_rolling_stats(df,lev, otimeheader, 30):.2f}%\",\n",
    "                        )\n",
    "\n",
    "                    with col4: \n",
    "                        st.metric(\n",
    "                            \"Win Rate\",\n",
    "                            f\"{row._4:.1f}%\",\n",
    "                        )\n",
    "                    with c4:\n",
    "                        st.metric(\n",
    "                        \"Rolling 90 Days\",\n",
    "                            \"\",#f\"{(1+get_rolling_stats(df,otimeheader, 30))*principal_balance:.2f}\",\n",
    "                            f\"{get_rolling_stats(df,lev, otimeheader, 90):.2f}%\",\n",
    "                        )\n",
    "                        st.metric(\n",
    "                        \"Rolling 180 Days\",\n",
    "                            \"\",#f\"{(1+get_rolling_stats(df,otimeheader, 30))*principal_balance:.2f}\",\n",
    "                            f\"{get_rolling_stats(df,lev, otimeheader, 180):.2f}%\",\n",
    "                        )\n",
    "                        \n",
    "    if submitted: \n",
    "        grouped_df = df.groupby('Exit Date').agg({'Signal':'min','Entry Date': 'min','Exit Date': 'max','Buy Price': 'mean',\n",
    "                                 'Sell Price' : 'max',\n",
    "                                 'P/L per token': 'mean', \n",
    "                                 'Calculated Return %' : lambda x: np.round(100*lev*x.sum(),2)})\n",
    "        grouped_df.index = range(1, len(grouped_df)+1)\n",
    "        grouped_df.rename(columns={'Buy Price':'Avg. Buy Price',\n",
    "                                   'P/L per token':'Avg. P/L per token', \n",
    "                                   'Calculated Return %':'P/L %'}, inplace=True)        \n",
    "    else: \n",
    "        grouped_df = df.groupby('Exit Date').agg({'Signal':'min','Entry Date': 'min','Exit Date': 'max','Buy Price': 'mean',\n",
    "                                 'Sell Price' : 'max',\n",
    "                                 'P/L per token': 'mean', \n",
    "                                 'P/L %':lambda x: np.round(x.sum()/4,2)})\n",
    "        grouped_df.index = range(1, len(grouped_df)+1)\n",
    "        grouped_df.rename(columns={'Buy Price':'Avg. Buy Price',\n",
    "                                   'P/L per token':'Avg. P/L per token'}, inplace=True)\n",
    "    st.subheader(\"Trade Logs\")\n",
    "    st.dataframe(grouped_df.style.format({'Avg. Buy Price': '${:.2f}', 'Sell Price': '${:.2f}', 'Avg. P/L per token':'${:.2f}', 'P/L %':'{:.2f}%'})\\\n",
    "    .applymap(my_style,subset=['Avg. P/L per token'])\\\n",
    "    .applymap(my_style,subset=['P/L %']), use_container_width=True)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    st.set_page_config(\n",
    "        \"Trading Bot Dashboard\",\n",
    "        layout=\"wide\",\n",
    "    )\n",
    "    runapp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b787b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bbytes] *",
   "language": "python",
   "name": "conda-env-bbytes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
